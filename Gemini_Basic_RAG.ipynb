{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9fbf60-3892-46c1-930c-2f62e077e2db",
   "metadata": {},
   "source": [
    "## Pdf Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582a2f96-8d3e-47a1-adb1-01aa72659fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Resat-Nuri-Guntekin-Calikusu.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf611d8-8208-4539-98ac-180144e81c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf1af1-fd0c-4d5a-a758-75ee71c18685",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "* In this way, we divide the document into smaller sections and access the correct information more easily.\n",
    "* In many RAG-based projects, the chunk size is chosen between 512 and 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae2d7e6-70eb-4b95-9801-43c3010ab153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 935\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Total\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20270c00-23f4-417f-bea5-c3758de6ea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'calibre 1.22.0 [http://calibre-ebook.com]', 'creator': 'calibre 1.22.0 [http://calibre-ebook.com]', 'creationdate': '2018-06-07T15:24:58+00:00', 'author': 'Reşat Nuri Güntekin', 'keywords': 'Roman, Türk Klasik, Cumhuriyet Dönemi', 'title': 'Çalıkuşu', 'source': 'Resat-Nuri-Guntekin-Calikusu.pdf', 'total_pages': 298, 'page': 7, 'page_label': '8'}, page_content='sedasız, ağırbaşlı ve upuzun bir komşu.\\nÖte yanımda manastır terbiyesinin istediği serin ve mağrur loşluğu temin için yapılmışa benzeyen\\nve panjurları hiç açılmayan bir uzun pencere dururdu. Ehemmiyetli bir keşif yapmıştım. Göğsümü\\nsıraya yaslayıp çenemi biraz yukarı kaldırdığım vakit panjurların arasından gökyüzünün bir\\nparçasıyla bir büyük akasyanın yaprakları arasından tek bir apartman penceresi ve bir balkon\\nparmaklığı görünürdü\\nDoğrusunu söylemek lâzım gelirse, manzara hiç de zengin değildi. Pencere her zaman kapalı durur,\\nbalkon parmaklığına hemen daima bir ufak çocuk şiltesi ile yorgan asılırdı.\\nFakat ben, bu kadarından da memnundum.\\nDers esnasında ellerim çenemin altında kilitli, sör hocalarıma çok ruhani görünmesi gereken bir\\nvaziyette gözlerimi göğe -panjur aralıklarından görünen hakiki gökyüzüne- uydurduğum zaman, onlar\\nbunu bir uslanma başlangıcı sanarak sevinirlerdi. Ben de onları atlatarak bizden gizlemeye')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see some values in the document\n",
    "\n",
    "docs[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95f163-6832-49b9-8676-f7e3126f7d46",
   "metadata": {},
   "source": [
    "## Creating Embedding using Google Generative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e403e0-b8ed-4f6f-a0e9-2bca462ddc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014134909026324749,\n",
       " -0.022324152290821075,\n",
       " -0.054603420197963715,\n",
       " -0.006284549366682768,\n",
       " -0.03392402455210686]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# Test (Convert text to vector)\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f1349-f0c9-4c8e-a8e1-ddf561ac4a5f",
   "metadata": {},
   "source": [
    "## Recording on ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968824b0-aa2e-405c-867a-d43699c1d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414ef0f2-cde5-476c-9965-8bb6ccd3ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Similarity feature takes embedding vectors and finds the closest vectors in Chroma.\n",
    "# For example: When we say \"What is encoder?\", it brings up the documents that contain the concept of \"encoder\" or are closest to it.\n",
    "# The value in the search results tells us how many values will be returned.\n",
    "# For imstance: If k=10, it brings us the 10 most relevant values.\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27880c07-89d2-43a4-a32c-dc90f9a5b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Feride kimdir?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5949dbac-89ba-4a7c-b430-1ade0a60dbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"retrieved_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52b4d4b-b356-409a-9049-285c4a354236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Teyze çocukları hemen hemen kardeş demektir. Feride’nin erkek kardeşi olmadığı için sen,\n",
      "doğrudan doğruya onun ağabeyi sayılırsın Kâmran; kardeşine “Hoş geldin,” desene!...\n",
      "Kâmran hâlâ bir şey söyleyemiyordu. Hafifçe eğildi, Feride’nin saçlarına dudaklarını dokundurdu.\n",
      "Sonra kulağına söyler gibi gayet yavaş:\n",
      "-Sizi tekrar görmek memnuniyetini söyleyebilmek için kelime bulamayacağım Feride Hanım, dedi.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931f2c3-978d-47ea-aa4a-c7dc2ae0d0a4",
   "metadata": {},
   "source": [
    "### LLM invoke operations using Google Gemini API structure\n",
    "\n",
    " *  **Max_tokens:** Max_tokens is the maximum length of the response produced by the model.\n",
    "\n",
    " *  **Temperature:** Temperature determines the randomness and creativity in the model's responses.\n",
    "\n",
    "**Temperature Value Range:** \n",
    "\n",
    "* **Low Values(0.1-0.3)**: It allows us to give more precise and consistent answers.\n",
    "\n",
    "* **Medium Values(0.4-0.7)**: It gives both logical and creative answers.\n",
    "\n",
    "* **High Values(0.8-1.0)**: It may give more random and creative answers and sometimes perhaps inconsistent answers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b994f003-c73c-4ceb-97d6-fcb3e3a54945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a04a40-c2ed-4ee7-8c9c-91aeb0fdc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86a425bf-bdf2-4b29-a450-8cb7acdce527",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ( \"You are an assistant for question-answering tasks\"\n",
    "                  \"Use the following pieces of retrieved context to answer\"\n",
    "                  \"If you don't know the answer, say that you don't know\"\n",
    "                  \"Use three sentences maximum and keep the answer corrects.\"\n",
    "                  \"\\n\\n\"\n",
    "                  \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d120371-a897-4e41-82a3-7bafb11ac623",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "       (\"system\", system_prompt),\n",
    "       (\"human\",  \"{input}\")\n",
    "        \n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f66976-0201-4810-a0e6-59d6c9b40535",
   "metadata": {},
   "source": [
    "## Creating a Question-Answer Chain (LLM + Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4fef489-9134-4c4b-bd64-11546201068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefa1d4-0dc0-4711-9412-440a5a4b2704",
   "metadata": {},
   "source": [
    "## Creating a RAG Chain (RAG + LLM Merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1782aed-9ae5-4418-8dcf-b46f1cccbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a863b-5c65-4553-abea-11953cf7be65",
   "metadata": {},
   "source": [
    "## Generating Answers by Running a User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c9f86cc-c5f4-45d2-b199-b119a32059d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Feride kimdir? Karakter özellikleri nedir?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d7b80e1-bdec-4b67-af18-ba0a3634c3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feride, Çalıkuşu romanının ana karakteridir.  Enerjik, şakacı, eğlenceli, merhametli ve biraz da çocuksu bir kişiliğe sahiptir.  Aynı zamanda fevri,  duygusal ve  idealisttir.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10066506-47b5-4d11-ab67-b025400df600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rag_env]",
   "language": "python",
   "name": "conda-env-rag_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
